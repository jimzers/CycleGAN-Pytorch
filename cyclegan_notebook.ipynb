{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "resnet_leaky_relu = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetBlock(nn.Module):\n",
    "    def __init__(self, batch_size=\"pog\", channels=256, alpha=0.01):\n",
    "        super(ResNetBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=channels, out_channels=channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=channels, out_channels=channels, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        self.bnorm1 = nn.BatchNorm2d(channels)\n",
    "        self.bnorm2 = nn.BatchNorm2d(channels)\n",
    "        \n",
    "        self.lrelu1 = nn.LeakyReLU(alpha)\n",
    "        \n",
    "    def forward(self, _input):\n",
    "        x = self.conv1(_input)\n",
    "        # todo: try relu before batch norm\n",
    "        x = self.bnorm1(x)\n",
    "        x = self.lrelu1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bnorm2(x)\n",
    "        \n",
    "        res = x + _input\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, batch_size=\"pog\", channels=3, img_height=256, img_width=256):\n",
    "        super(Generator, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=7, stride=1, padding=3)\n",
    "        self.conv2 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "        self.bnorm1 = nn.BatchNorm2d(64)\n",
    "        self.bnorm2 = nn.BatchNorm2d(128)\n",
    "        self.bnorm3 = nn.BatchNorm2d(256)\n",
    "        \n",
    "        self.rblock1 = ResNetBlock(batch_size, 256)\n",
    "        self.rblock2 = ResNetBlock(batch_size, 256)\n",
    "        self.rblock3 = ResNetBlock(batch_size, 256)\n",
    "        self.rblock4 = ResNetBlock(batch_size, 256)\n",
    "        self.rblock5 = ResNetBlock(batch_size, 256)\n",
    "        self.rblock6 = ResNetBlock(batch_size, 256)\n",
    "        self.rblock7 = ResNetBlock(batch_size, 256)\n",
    "        self.rblock8 = ResNetBlock(batch_size, 256)\n",
    "        self.rblock9 = ResNetBlock(batch_size, 256)\n",
    "        \n",
    "        self.conv_trans1 = nn.ConvTranspose2d(in_channels=256, out_channels=128, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        self.conv_trans2 = nn.ConvTranspose2d(in_channels=128, out_channels=64, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        self.final_conv = nn.Conv2d(in_channels=64, out_channels=3, kernel_size=7, stride=1, padding=3)\n",
    "        \n",
    "        self.bnorm4 = nn.BatchNorm2d(128)\n",
    "        self.bnorm5 = nn.BatchNorm2d(64)\n",
    "        \n",
    "    def forward(self, _input):\n",
    "        x = self.conv1(_input)\n",
    "        x = F.leaky_relu(self.bnorm1(x), negative_slope=0.01)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = F.leaky_relu(self.bnorm2(x), negative_slope=0.01)\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x = F.leaky_relu(self.bnorm3(x), negative_slope=0.01)\n",
    "        \n",
    "        x = self.rblock1(x)\n",
    "        x = self.rblock2(x)\n",
    "        x = self.rblock3(x)\n",
    "        x = self.rblock4(x)\n",
    "        x = self.rblock5(x)\n",
    "        x = self.rblock6(x)\n",
    "        x = self.rblock7(x)\n",
    "        x = self.rblock8(x)\n",
    "        x = self.rblock9(x)\n",
    "        \n",
    "        x = self.conv_trans1(x)\n",
    "        x = F.leaky_relu(self.bnorm4(x), negative_slope=0.01)\n",
    "        \n",
    "        x = self.conv_trans2(x)\n",
    "        x = F.leaky_relu(self.bnorm5(x), negative_slope=0.01)\n",
    "        \n",
    "        x = self.final_conv(x)\n",
    "        \n",
    "        res = F.tanh(x)\n",
    "        \n",
    "        return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 256, 256]           9,472\n",
      "       BatchNorm2d-2         [-1, 64, 256, 256]             128\n",
      "            Conv2d-3        [-1, 128, 128, 128]          73,856\n",
      "       BatchNorm2d-4        [-1, 128, 128, 128]             256\n",
      "            Conv2d-5          [-1, 256, 64, 64]         295,168\n",
      "       BatchNorm2d-6          [-1, 256, 64, 64]             512\n",
      "            Conv2d-7          [-1, 256, 64, 64]         590,080\n",
      "       BatchNorm2d-8          [-1, 256, 64, 64]             512\n",
      "         LeakyReLU-9          [-1, 256, 64, 64]               0\n",
      "           Conv2d-10          [-1, 256, 64, 64]         590,080\n",
      "      BatchNorm2d-11          [-1, 256, 64, 64]             512\n",
      "      ResNetBlock-12          [-1, 256, 64, 64]               0\n",
      "           Conv2d-13          [-1, 256, 64, 64]         590,080\n",
      "      BatchNorm2d-14          [-1, 256, 64, 64]             512\n",
      "        LeakyReLU-15          [-1, 256, 64, 64]               0\n",
      "           Conv2d-16          [-1, 256, 64, 64]         590,080\n",
      "      BatchNorm2d-17          [-1, 256, 64, 64]             512\n",
      "      ResNetBlock-18          [-1, 256, 64, 64]               0\n",
      "           Conv2d-19          [-1, 256, 64, 64]         590,080\n",
      "      BatchNorm2d-20          [-1, 256, 64, 64]             512\n",
      "        LeakyReLU-21          [-1, 256, 64, 64]               0\n",
      "           Conv2d-22          [-1, 256, 64, 64]         590,080\n",
      "      BatchNorm2d-23          [-1, 256, 64, 64]             512\n",
      "      ResNetBlock-24          [-1, 256, 64, 64]               0\n",
      "           Conv2d-25          [-1, 256, 64, 64]         590,080\n",
      "      BatchNorm2d-26          [-1, 256, 64, 64]             512\n",
      "        LeakyReLU-27          [-1, 256, 64, 64]               0\n",
      "           Conv2d-28          [-1, 256, 64, 64]         590,080\n",
      "      BatchNorm2d-29          [-1, 256, 64, 64]             512\n",
      "      ResNetBlock-30          [-1, 256, 64, 64]               0\n",
      "           Conv2d-31          [-1, 256, 64, 64]         590,080\n",
      "      BatchNorm2d-32          [-1, 256, 64, 64]             512\n",
      "        LeakyReLU-33          [-1, 256, 64, 64]               0\n",
      "           Conv2d-34          [-1, 256, 64, 64]         590,080\n",
      "      BatchNorm2d-35          [-1, 256, 64, 64]             512\n",
      "      ResNetBlock-36          [-1, 256, 64, 64]               0\n",
      "           Conv2d-37          [-1, 256, 64, 64]         590,080\n",
      "      BatchNorm2d-38          [-1, 256, 64, 64]             512\n",
      "        LeakyReLU-39          [-1, 256, 64, 64]               0\n",
      "           Conv2d-40          [-1, 256, 64, 64]         590,080\n",
      "      BatchNorm2d-41          [-1, 256, 64, 64]             512\n",
      "      ResNetBlock-42          [-1, 256, 64, 64]               0\n",
      "           Conv2d-43          [-1, 256, 64, 64]         590,080\n",
      "      BatchNorm2d-44          [-1, 256, 64, 64]             512\n",
      "        LeakyReLU-45          [-1, 256, 64, 64]               0\n",
      "           Conv2d-46          [-1, 256, 64, 64]         590,080\n",
      "      BatchNorm2d-47          [-1, 256, 64, 64]             512\n",
      "      ResNetBlock-48          [-1, 256, 64, 64]               0\n",
      "           Conv2d-49          [-1, 256, 64, 64]         590,080\n",
      "      BatchNorm2d-50          [-1, 256, 64, 64]             512\n",
      "        LeakyReLU-51          [-1, 256, 64, 64]               0\n",
      "           Conv2d-52          [-1, 256, 64, 64]         590,080\n",
      "      BatchNorm2d-53          [-1, 256, 64, 64]             512\n",
      "      ResNetBlock-54          [-1, 256, 64, 64]               0\n",
      "           Conv2d-55          [-1, 256, 64, 64]         590,080\n",
      "      BatchNorm2d-56          [-1, 256, 64, 64]             512\n",
      "        LeakyReLU-57          [-1, 256, 64, 64]               0\n",
      "           Conv2d-58          [-1, 256, 64, 64]         590,080\n",
      "      BatchNorm2d-59          [-1, 256, 64, 64]             512\n",
      "      ResNetBlock-60          [-1, 256, 64, 64]               0\n",
      "  ConvTranspose2d-61        [-1, 128, 128, 128]         295,040\n",
      "      BatchNorm2d-62        [-1, 128, 128, 128]             256\n",
      "  ConvTranspose2d-63         [-1, 64, 256, 256]          73,792\n",
      "      BatchNorm2d-64         [-1, 64, 256, 256]             128\n",
      "           Conv2d-65          [-1, 3, 256, 256]           9,411\n",
      "================================================================\n",
      "Total params: 11,388,675\n",
      "Trainable params: 11,388,675\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.75\n",
      "Forward/backward pass size (MB): 641.50\n",
      "Params size (MB): 43.44\n",
      "Estimated Total Size (MB): 685.69\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jimzers/coding/cyclegan-pytorch/venv/lib/python3.6/site-packages/torch/nn/functional.py:1558: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    }
   ],
   "source": [
    "sanity_check_generator_model = Generator().to(device)\n",
    "summary(sanity_check_generator_model, input_size=(3, 256, 256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    \"\"\"\n",
    "    pix2pix discriminator\n",
    "    \"\"\"\n",
    "    def __init__(self, batch_size=\"pog\", channels=3, img_height=256, img_width=256):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=4, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=4, stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=4, stride=2, padding=1)\n",
    "        self.conv4 = nn.Conv2d(in_channels=256, out_channels=512, kernel_size=4, stride=1, padding=1)\n",
    "        self.conv5 = nn.Conv2d(in_channels=512, out_channels=1, kernel_size=4, stride=1, padding=1)\n",
    "        \n",
    "        self.bnorm1 = nn.BatchNorm2d(64)\n",
    "        self.bnorm2 = nn.BatchNorm2d(128)\n",
    "        self.bnorm3 = nn.BatchNorm2d(256)\n",
    "        self.bnorm4 = nn.BatchNorm2d(512)\n",
    "        \n",
    "    def forward(self, _input):\n",
    "        x = self.conv1(_input)\n",
    "        x = F.leaky_relu(self.bnorm1(x), negative_slope=0.01)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = F.leaky_relu(self.bnorm2(x), negative_slope=0.01)\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x = F.leaky_relu(self.bnorm3(x), negative_slope=0.01)\n",
    "        \n",
    "        x = self.conv4(x)\n",
    "        x = F.leaky_relu(self.bnorm4(x), negative_slope=0.01)\n",
    "        \n",
    "        res = self.conv5(x)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 128, 128]           3,136\n",
      "       BatchNorm2d-2         [-1, 64, 128, 128]             128\n",
      "            Conv2d-3          [-1, 128, 64, 64]         131,200\n",
      "       BatchNorm2d-4          [-1, 128, 64, 64]             256\n",
      "            Conv2d-5          [-1, 256, 32, 32]         524,544\n",
      "       BatchNorm2d-6          [-1, 256, 32, 32]             512\n",
      "            Conv2d-7          [-1, 512, 31, 31]       2,097,664\n",
      "       BatchNorm2d-8          [-1, 512, 31, 31]           1,024\n",
      "            Conv2d-9            [-1, 1, 30, 30]           8,193\n",
      "================================================================\n",
      "Total params: 2,766,657\n",
      "Trainable params: 2,766,657\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.75\n",
      "Forward/backward pass size (MB): 35.51\n",
      "Params size (MB): 10.55\n",
      "Estimated Total Size (MB): 46.82\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "sanity_check_discriminator_model = Discriminator().to(device)\n",
    "summary(sanity_check_discriminator_model, input_size=(3, 256, 256))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start by making a regular training loop, and then turn it into a class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Improvements: Explore using detach to make it possible to just do all the grad steps with one single cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# TODO: send to device. after debugging loop\n",
    "g_az = Generator()\n",
    "g_za = Generator()\n",
    "\n",
    "d_a = Discriminator()\n",
    "d_z = Discriminator()\n",
    "\n",
    "loss_gan = nn.MSELoss()\n",
    "loss_discrim = nn.L1Loss()\n",
    "loss_cycle = nn.L1Loss()\n",
    "loss_identity_mult = nn.L1Loss()\n",
    "\n",
    "g_params = list(g_az.parameters()) + list(g_za.parameters())\n",
    "d_params = list(d_a.parameters()) + list(d_z.parameters())\n",
    "\n",
    "g_opt = Adam(g_params, lr=0.001)\n",
    "d_opt = Adam(d_params, lr=0.001)\n",
    "\n",
    "loss_gan_mult = 5\n",
    "loss_cycle_mult = 10\n",
    "loss_identity_mult = 1\n",
    "\n",
    "\n",
    "def train(batch):\n",
    "    \"\"\"\n",
    "    single step training\n",
    "    \"\"\"\n",
    "    a_imgs_real = batch['a']\n",
    "    z_imgs_real = batch['z']\n",
    "\n",
    "\n",
    "    # Discriminator Training\n",
    "    # note: can you run detach on generator outputs here?\n",
    "    \n",
    "    d_opt.zero_grad()\n",
    "    # real images\n",
    "    guess_d_a = d_a(a_imgs_real)\n",
    "    d_a_same_loss = torch.mean((1 - guess_d_a)**2)\n",
    "    \n",
    "    guess_d_z = d_z(z_imgs_real)\n",
    "    d_z_same_loss = torch.mean((1 - guess_d_z)**2)\n",
    "    \n",
    "    total_d_real_loss = d_a_same_loss + d_z_same_loss  # TODO: MULTIPLY HERE\n",
    "    \n",
    "    total_d_real_loss.backward()\n",
    "    d_opt.step()\n",
    "    \n",
    "    d_opt.zero_grad()\n",
    "    \n",
    "    # generated fake images\n",
    "    generated_img_a = g_za(z_imgs_real)\n",
    "    guess_d_a = d_a(generated_img_a)  # for the resultant of a to z\n",
    "    d_a_fake_loss = torch.mean((guess_d_a)**2)  # same as (0 - generated_img_a)**2\n",
    "    \n",
    "    generated_img_z = g_az(a_imgs_real)\n",
    "    guess_d_z = d_z(generated_img_z)\n",
    "    d_z_fake_loss = torch.mean((guess_d_z)**2)\n",
    "    \n",
    "    total_d_fake_loss = d_a_fake_loss + d_z_fake_loss  # TODO: MULTIPLY HERE\n",
    "    \n",
    "    total_d_fake_loss.backward()\n",
    "    d_opt.step()\n",
    "    \n",
    "    \n",
    "    # Generator Training\n",
    "    \n",
    "    g_opt.zero_grad()\n",
    "    # a - z - a cycle loss (and also generator loss)\n",
    "    generated_img_z = g_az(a_imgs_real)\n",
    "    guess_d_z = d_z(generated_img_z)\n",
    "    \n",
    "    g_az_loss = torch.mean((1 - guess_d_z)**2)  # TODO: MULTIPLY HERE\n",
    "    # continue the cycle\n",
    "    generated_cycled_img_a = g_za(generated_img_z)\n",
    "    \n",
    "    aza_cycle_loss = torch.mean((a_imgs_real - generated_cycled_img_a)**2)\n",
    "    \n",
    "    total_aza_loss = g_az_loss + aza_cycle_loss\n",
    "    total_aza_loss.backward()\n",
    "    g_opt.step()\n",
    "    \n",
    "    g_opt.zero_grad()\n",
    "    # z - a - z cycle loss (and also generator loss)\n",
    "    generated_img_a = g_za(z_imgs_real)\n",
    "    guess_d_a = d_a(generated_img_a)\n",
    "    \n",
    "    g_za_loss = torch.mean((1 - guess_d_a)**2)  # TODO: MULTIPLY HERE\n",
    "    # continue the cycle\n",
    "    generated_cycled_img_z = g_az(generated_img_a)\n",
    "    \n",
    "    zaz_cycle_loss = torch.mean((z_imgs_real - generated_cycled_img_z)**2)\n",
    "    \n",
    "    total_zaz_loss = g_za_loss + zaz_cycle_loss  # TODO: MULTIPLY HERE\n",
    "    total_zaz_loss.backward()\n",
    "    g_opt.step()\n",
    "    \n",
    "    \n",
    "    # Identity Loss\n",
    "    g_opt.zero_grad()\n",
    "    \n",
    "    generated_identity_img_z = g_az(z_imgs_real)\n",
    "    identity_loss_az = torch.mean(torch.abs(z_imgs_real - generated_identity_img_z))\n",
    "    \n",
    "    generated_identity_img_a = g_za(a_imgs_real)\n",
    "    identity_loss_za = torch.mean(torch.abs(a_imgs_real - generated_identity_img_a))\n",
    "    \n",
    "    total_identity_loss = identity_loss_az + identity_loss_za  # TODO: MULTIPLY HERE (by 1 lol)\n",
    "    \n",
    "    total_identity_loss.backward()\n",
    "    g_opt.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO:\n",
    "\n",
    "- Add multiplication factors to losses\n",
    "- Add dataloader + generator\n",
    "- Add image buffer (once main loop works)\n",
    "\n",
    "data directory structure (from root)\n",
    "\n",
    "```\n",
    "--> data\n",
    " |\n",
    " ---> a_dir\n",
    " |\n",
    " ---> z_dir\n",
    " \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import glob\n",
    "import os\n",
    "import random\n",
    "\n",
    "class CycleGANDataset(Dataset):\n",
    "    def __init__(self, a_dir, z_dir, file_extension='*.jpg', transform=None, aligned=False):\n",
    "        super(CycleGANDataset, self).__init__()\n",
    "        self.a_arr = glob.glob(os.path.join(a_dir, file_extension))\n",
    "        self.z_arr = glob.glob(os.path.join(z_dir, file_extension))\n",
    "        self.a_dir = a_dir\n",
    "        self.z_dir = z_dir\n",
    "        self.transform = transform\n",
    "        self.aligned = aligned\n",
    "    \n",
    "    def __len__(self):\n",
    "        # could be a arr or z arr, shouldn't matter\n",
    "        return max(len(self.a_arr), len(self.z_arr))\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Generate one sample of data, based on dirty data\"\"\"\n",
    "        \n",
    "        # grab a random index for each. the modulus allows overflow, and makes it unaligned if overflow\n",
    "        idx_a = index % len(self.a_arr)\n",
    "        if self.aligned:\n",
    "            idx_z = index % len(self.z_arr)\n",
    "        else:\n",
    "            idx_z = random.randint(0, len(self.z_arr) - 1)\n",
    "        \n",
    "        a_img_name = os.path.basename(self.a_arr[idx_a])\n",
    "        z_img_name = os.path.basename(self.z_arr[idx_z])\n",
    "        a_path = os.path.join(self.a_dir, a_img_name)\n",
    "        z_path = os.path.join(self.z_dir, z_img_name)\n",
    "        transformed_a = Image.open(a_path)\n",
    "        transformed_z = Image.open(z_path)\n",
    "        \n",
    "        if self.transform:\n",
    "            # notice how with each transform, they are each independent.\n",
    "            # this allows the random crop and flips to be different with each img\n",
    "            # because cyclegan is meant for unpaired it won't matter\n",
    "            transformed_a = self.transform(transformed_a)\n",
    "            transformed_z = self.transform(transformed_z)\n",
    "        \n",
    "        return {'a': transformed_a, 'z': transformed_z}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "img_h = 256\n",
    "img_w = 256\n",
    "\n",
    "trans = transforms.Compose([\n",
    "    transforms.Resize((int(img_h*1.12),int(img_w*1.12)), Image.BICUBIC), # make it bigger so random crop is more random\n",
    "    transforms.RandomCrop((img_h, img_w)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,)) # mean, std of each channel. can provice a tuple for 3 dim images\n",
    "])\n",
    "training_set = CycleGANDataset(a_dir='./data/a_dir', z_dir='./data/z_dir', transform=trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "params = {'batch_size': 2,\n",
    "          'shuffle': True,\n",
    "          'num_workers': 1}\n",
    "\n",
    "training_generator = DataLoader(training_set, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./data/a_dir/IMG_5267.jpg',\n",
       " './data/a_dir/IMG_5268.jpg',\n",
       " './data/a_dir/IMG_5269.jpg',\n",
       " './data/a_dir/koalastare.jpg',\n",
       " './data/a_dir/nautilus.jpg',\n",
       " './data/a_dir/drone.jpg']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glob.glob(os.path.join('./data/a_dir', '*.jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n",
      "hi\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 2\n",
    "for epoch in range(0, n_epochs):\n",
    "    print(\"hi\")\n",
    "    for i, batch in enumerate(training_generator):\n",
    "        train(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# TODO: send to device. after debugging loop\n",
    "g_az = Generator().to(device)\n",
    "g_za = Generator().to(device)\n",
    "\n",
    "d_a = Discriminator().to(device)\n",
    "d_z = Discriminator().to(device)\n",
    "\n",
    "loss_gan = nn.MSELoss()\n",
    "loss_discrim = nn.L1Loss()\n",
    "loss_cycle = nn.L1Loss()\n",
    "loss_identity_mult = nn.L1Loss()\n",
    "\n",
    "g_params = list(g_az.parameters()) + list(g_za.parameters())\n",
    "d_params = list(d_a.parameters()) + list(d_z.parameters())\n",
    "\n",
    "g_opt = Adam(g_params, lr=0.001)\n",
    "d_opt = Adam(d_params, lr=0.001)\n",
    "\n",
    "loss_gan_mult = 5\n",
    "loss_cycle_mult = 10\n",
    "loss_identity_mult = 1\n",
    "\n",
    "\n",
    "def train_gpu(batch):\n",
    "    \"\"\"\n",
    "    single step training\n",
    "    \"\"\"\n",
    "    a_imgs_real = batch['a'].to(device)\n",
    "    z_imgs_real = batch['z'].to(device)\n",
    "\n",
    "\n",
    "    # Discriminator Training\n",
    "    # note: can you run detach on generator outputs here?\n",
    "    \n",
    "    d_opt.zero_grad()\n",
    "    # real images\n",
    "    guess_d_a = d_a(a_imgs_real)\n",
    "    d_a_same_loss = torch.mean((1 - guess_d_a)**2)\n",
    "    \n",
    "    guess_d_z = d_z(z_imgs_real)\n",
    "    d_z_same_loss = torch.mean((1 - guess_d_z)**2)\n",
    "    \n",
    "    total_d_real_loss = d_a_same_loss + d_z_same_loss  # TODO: MULTIPLY HERE\n",
    "    \n",
    "    total_d_real_loss.backward()\n",
    "    d_opt.step()\n",
    "    \n",
    "    d_opt.zero_grad()\n",
    "    \n",
    "    # generated fake images\n",
    "    generated_img_a = g_za(z_imgs_real)\n",
    "    guess_d_a = d_a(generated_img_a)  # for the resultant of a to z\n",
    "    d_a_fake_loss = torch.mean((guess_d_a)**2)  # same as (0 - generated_img_a)**2\n",
    "    \n",
    "    generated_img_z = g_az(a_imgs_real)\n",
    "    guess_d_z = d_z(generated_img_z)\n",
    "    d_z_fake_loss = torch.mean((guess_d_z)**2)\n",
    "    \n",
    "    total_d_fake_loss = d_a_fake_loss + d_z_fake_loss  # TODO: MULTIPLY HERE\n",
    "    \n",
    "    total_d_fake_loss.backward()\n",
    "    d_opt.step()\n",
    "    \n",
    "    \n",
    "    # Generator Training\n",
    "    \n",
    "    g_opt.zero_grad()\n",
    "    # a - z - a cycle loss (and also generator loss)\n",
    "    generated_img_z = g_az(a_imgs_real)\n",
    "    guess_d_z = d_z(generated_img_z)\n",
    "    \n",
    "    g_az_loss = torch.mean((1 - guess_d_z)**2)  # TODO: MULTIPLY HERE\n",
    "    # continue the cycle\n",
    "    generated_cycled_img_a = g_za(generated_img_z)\n",
    "    \n",
    "    aza_cycle_loss = torch.mean((a_imgs_real - generated_cycled_img_a)**2)\n",
    "    \n",
    "    total_aza_loss = g_az_loss + aza_cycle_loss\n",
    "    total_aza_loss.backward()\n",
    "    g_opt.step()\n",
    "    \n",
    "    g_opt.zero_grad()\n",
    "    # z - a - z cycle loss (and also generator loss)\n",
    "    generated_img_a = g_za(z_imgs_real)\n",
    "    guess_d_a = d_a(generated_img_a)\n",
    "    \n",
    "    g_za_loss = torch.mean((1 - guess_d_a)**2)  # TODO: MULTIPLY HERE\n",
    "    # continue the cycle\n",
    "    generated_cycled_img_z = g_az(generated_img_a)\n",
    "    \n",
    "    zaz_cycle_loss = torch.mean((z_imgs_real - generated_cycled_img_z)**2)\n",
    "    \n",
    "    total_zaz_loss = g_za_loss + zaz_cycle_loss  # TODO: MULTIPLY HERE\n",
    "    total_zaz_loss.backward()\n",
    "    g_opt.step()\n",
    "    \n",
    "    \n",
    "    # Identity Loss\n",
    "    g_opt.zero_grad()\n",
    "    \n",
    "    generated_identity_img_z = g_az(z_imgs_real)\n",
    "    identity_loss_az = torch.mean(torch.abs(z_imgs_real - generated_identity_img_z))\n",
    "    \n",
    "    generated_identity_img_a = g_za(a_imgs_real)\n",
    "    identity_loss_za = torch.mean(torch.abs(a_imgs_real - generated_identity_img_a))\n",
    "    \n",
    "    total_identity_loss = identity_loss_az + identity_loss_za  # TODO: MULTIPLY HERE (by 1 lol)\n",
    "    \n",
    "    total_identity_loss.backward()\n",
    "    g_opt.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n",
      "hi\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 2\n",
    "for epoch in range(0, n_epochs):\n",
    "    print(\"hi\")\n",
    "    for i, batch in enumerate(training_generator):\n",
    "        train_gpu(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
